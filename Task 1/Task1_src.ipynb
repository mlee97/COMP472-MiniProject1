{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4085af6f-ff5c-4c9a-8591-b2e9c03c5462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import math\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "#-------------Step 2: Plot instances per class-------------#\n",
    "\n",
    "#Create a list of strings that contains the 5 classes in the BBC News Site.\n",
    "ctg = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "#Create an empty list to store the number of instances per class.\n",
    "numFilesPerCtg = []\n",
    "\n",
    "#Count Files in Business Directory\n",
    "business_folder_path = 'BBC/business'\n",
    "dirListing = os.listdir(business_folder_path)\n",
    "numFilesPerCtg.append(len(dirListing))\n",
    "\n",
    "#Count Files in Entertainment Directory\n",
    "entertainment_folder_path = 'BBC/entertainment'\n",
    "dirListing = os.listdir(entertainment_folder_path)\n",
    "numFilesPerCtg.append(len(dirListing))\n",
    "\n",
    "#Count Files in Politics Directory\n",
    "politics_folder_path = 'BBC/politics'\n",
    "dirListing = os.listdir(politics_folder_path)\n",
    "numFilesPerCtg.append(len(dirListing))\n",
    "\n",
    "#Count Files in Sport Directory\n",
    "sport_folder_path = 'BBC/sport'\n",
    "dirListing = os.listdir(sport_folder_path)\n",
    "numFilesPerCtg.append(len(dirListing))\n",
    "\n",
    "#Count Files in Tech Directory\n",
    "tech_folder_path = 'BBC/tech'\n",
    "dirListing = os.listdir(tech_folder_path)\n",
    "numFilesPerCtg.append(len(dirListing))\n",
    "\n",
    "#Plot bar graph of the number of instances per class.\n",
    "plt.bar(ctg, numFilesPerCtg)\n",
    "plt.xlabel('class')\n",
    "plt.ylabel('instances per class')\n",
    "\n",
    "#Save graph to a PDF file\n",
    "plt.savefig(\"BBC-distribution.pdf\")\n",
    "\n",
    "#-------------Step3: Load dataset-------------#\n",
    "dataset = load_files(\"BBC\", categories = ctg, encoding = 'latin1')\n",
    "\n",
    "#-------------Step4: Pre-process the dataset-------------#\n",
    "vectorizer = CountVectorizer()\n",
    "cv_fit = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "#-------------Step5: Split into Training and Testing Set-------------#\n",
    "X_train, X_test, y_train, y_test = train_test_split(cv_fit, dataset.target, train_size=0.80, test_size=0.20, random_state=None)\n",
    "\n",
    "#-------------Step6: Train a multinomial Naive Bayes Classifier on the training set-------------#\n",
    "multinomialNaiveBayes = MultinomialNB()\n",
    "multinomialNaiveBayes.fit(X_train, y_train)\n",
    "y_predict = multinomialNaiveBayes.predict(X_test)\n",
    "\n",
    "#-------------Step7: BBC Performance-------------#\n",
    "\n",
    "#open txt file\n",
    "open(\"bbc-performance.txt\", \"w\").close()\n",
    "performance_file = open('bbc-performance.txt', 'a')\n",
    "\n",
    "#--(a)A clear seperator describing the model--#\n",
    "performance_file.write('-------------------------------------\\n')\n",
    "performance_file.write('Multi-nomialNB default values, try 1\\n')\n",
    "performance_file.write('-------------------------------------\\n')\n",
    "\n",
    "#--(b)Confusion Matrix--#\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "performance_file.write('(b)\\n')\n",
    "for item in cm:\n",
    "        performance_file.write(\"%s\\n\" % item)\n",
    "performance_file.write('\\n')\n",
    "\n",
    "#--(c)Find Precision, Recall, F1-measure for each class--#\n",
    "target_names = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "performance_file.write('(c)\\n')\n",
    "performance_file.write(classification_report(y_test, y_predict, target_names=target_names))\n",
    "performance_file.write('\\n')\n",
    "\n",
    "#--(d)Find Accuracy, Macro-Average F1 and Weighted-Average F1 of the moodel--#\n",
    "accuracy = str(accuracy_score(y_test, y_predict))\n",
    "performance_file.write('(d)\\n')\n",
    "performance_file.write('accuracy: ' + accuracy + \"\\n\")\n",
    "\n",
    "macro = str(f1_score(y_test, y_predict, average='macro'))\n",
    "performance_file.write('macro-average F1: ' + macro + \"\\n\")\n",
    "\n",
    "weighted = str(f1_score(y_test, y_predict, average='weighted'))\n",
    "performance_file.write('weighted-average F1: ' + weighted + \"\\n\\n\")\n",
    "\n",
    "#--(e)Find prior probability of each class--#\n",
    "priors = multinomialNaiveBayes.class_log_prior_\n",
    "performance_file.write('(e)\\n')\n",
    "performance_file.write(\"\\nP(business)= \" + str(priors[0]))\n",
    "performance_file.write(\"\\nP(entertainment)= \" + str(priors[1]))\n",
    "performance_file.write(\"\\nP(politics)= \" + str(priors[2]))\n",
    "performance_file.write(\"\\nP(sport)= \" + str(priors[3]))\n",
    "performance_file.write(\"\\nP(tech)= \" + str(priors[4]))\n",
    "performance_file.write('\\n\\n')\n",
    "\n",
    "#--(f)The size of the vocabulary--#\n",
    "performance_file.write('(f)\\n')\n",
    "sizeVocab = multinomialNaiveBayes.n_features_in_\n",
    "performance_file.write('size of vocabulary: ' + str(sizeVocab) + '\\n\\n')\n",
    "\n",
    "#--(g)The number of word-tokens in each class--#\n",
    "performance_file.write('(g)\\n')\n",
    "#get word frequency per class\n",
    "wordFredPerClass = multinomialNaiveBayes.feature_count_\n",
    "performance_file.write('Number of words in business: ' + str(wordFredPerClass[0].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in entertainment: ' + str(wordFredPerClass[1].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in politics: ' + str(wordFredPerClass[2].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in sport: ' + str(wordFredPerClass[3].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in tech: ' + str(wordFredPerClass[4].sum(axis=0)) + '\\n\\n')\n",
    "\n",
    "#--(h)The number of word-tokens in the entire corpus--#\n",
    "performance_file.write('(h)\\n')\n",
    "wordFreq = cv_fit.toarray().sum(axis=0)\n",
    "performance_file.write('The number of word tokens for the entire corpus: ' + str(wordFreq.sum(axis=0)) + '\\n\\n')\n",
    "\n",
    "#--(i)The number and percentage of words with a frequency of zero in each class--#\n",
    "performance_file.write('(i)\\n')\n",
    "\n",
    "#count business\n",
    "\n",
    "#We know how many words are in business \n",
    "wordsInBusiness = wordFredPerClass[0].sum(axis=0)\n",
    "\n",
    "#And we can find the amount of zeros through subtraction\n",
    "#Take amount of articles x size of the vocabulary\n",
    "#Then subtract it by the amount of words that show up in business\n",
    "totalAmtWords = 510 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInBusiness\n",
    "performance_file.write('Number of words in business with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in business with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count entertainment\n",
    "wordsInEntertainment = wordFredPerClass[1].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 386 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInEntertainment\n",
    "performance_file.write('Number of words in entertainment with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in entertainment with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count politics\n",
    "wordsInPolitics = wordFredPerClass[2].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 417 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInPolitics\n",
    "performance_file.write('Number of words in politics with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in politics with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count sport\n",
    "wordsInSport = wordFredPerClass[3].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 511 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInSport\n",
    "performance_file.write('Number of words in sport with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in sport with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count tech\n",
    "wordsInTech = wordFredPerClass[4].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 401 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInTech\n",
    "performance_file.write('Number of words in tech with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in tech with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#--(j)The number and percentage of words with a frequency of one in the entire corpus--#\n",
    "performance_file.write('(j)\\n')\n",
    "performance_file.write('Number of words with frequency 1: ' + str(np.count_nonzero(cv_fit.toarray() == 1)) + \"\\n\")\n",
    "totalAmtWords = 2225 * sizeVocab\n",
    "performance_file.write('Percentage of words with frequency 1: ' + str(np.count_nonzero(cv_fit.toarray() == 1) / totalAmtWords) + \"\\n\\n\")\n",
    "\n",
    "#--(k)Your 2 favorite words (that are present in the vocabulary) and their log-prob--#\n",
    "performance_file.write('(k)\\n')\n",
    "#Sum up all words\n",
    "result = [sum(x) for x in zip(*cv_fit.toarray())]\n",
    "\n",
    "#Find the frequency of favorite words\n",
    "#'athletic' has index 3000\n",
    "athletic = result[3000]\n",
    "#'goalscorer' has index 12000\n",
    "goalscorer = result[12000]\n",
    "\n",
    "#Find probability of each word\n",
    "athleticProb = athletic/totalAmtWords\n",
    "goalscorerProb = goalscorer/totalAmtWords\n",
    "\n",
    "#Find log probability of each word\n",
    "performance_file.write('Log Probability of\\'athletic\\': ' + str(math.log(10,athleticProb)) + \"\\n\")\n",
    "performance_file.write('Log Probability of\\'athletic\\': ' + str(math.log(10,goalscorerProb)) + \"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#-------------Step6: Train a multinomial Naive Bayes Classifier on the training set-------------#\n",
    "multinomialNaiveBayes = MultinomialNB()\n",
    "multinomialNaiveBayes.fit(X_train, y_train)\n",
    "y_predict = multinomialNaiveBayes.predict(X_test)\n",
    "\n",
    "#-------------Step7: BBC Performance-------------#\n",
    "\n",
    "#--(a)A clear seperator describing the model--#\n",
    "performance_file.write('-------------------------------------\\n')\n",
    "performance_file.write('Multi-nomialNB default values, try 2\\n')\n",
    "performance_file.write('-------------------------------------\\n')\n",
    "\n",
    "#--(b)Confusion Matrix--#\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "performance_file.write('(b)\\n')\n",
    "for item in cm:\n",
    "        performance_file.write(\"%s\\n\" % item)\n",
    "performance_file.write('\\n')\n",
    "\n",
    "#--(c)Find Precision, Recall, F1-measure for each class--#\n",
    "target_names = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "performance_file.write('(c)\\n')\n",
    "performance_file.write(classification_report(y_test, y_predict, target_names=target_names))\n",
    "performance_file.write('\\n')\n",
    "\n",
    "#--(d)Find Accuracy, Macro-Average F1 and Weighted-Average F1 of the moodel--#\n",
    "accuracy = str(accuracy_score(y_test, y_predict))\n",
    "performance_file.write('(d)\\n')\n",
    "performance_file.write('accuracy: ' + accuracy + \"\\n\")\n",
    "\n",
    "macro = str(f1_score(y_test, y_predict, average='macro'))\n",
    "performance_file.write('macro-average F1: ' + macro + \"\\n\")\n",
    "\n",
    "weighted = str(f1_score(y_test, y_predict, average='weighted'))\n",
    "performance_file.write('weighted-average F1: ' + weighted + \"\\n\\n\")\n",
    "\n",
    "#--(e)Find prior probability of each class--#\n",
    "priors = multinomialNaiveBayes.class_log_prior_\n",
    "performance_file.write('(e)\\n')\n",
    "performance_file.write(\"\\nP(business)= \" + str(priors[0]))\n",
    "performance_file.write(\"\\nP(entertainment)= \" + str(priors[1]))\n",
    "performance_file.write(\"\\nP(politics)= \" + str(priors[2]))\n",
    "performance_file.write(\"\\nP(sport)= \" + str(priors[3]))\n",
    "performance_file.write(\"\\nP(tech)= \" + str(priors[4]))\n",
    "performance_file.write('\\n\\n')\n",
    "\n",
    "#--(f)The size of the vocabulary--#\n",
    "performance_file.write('(f)\\n')\n",
    "sizeVocab = multinomialNaiveBayes.n_features_in_\n",
    "performance_file.write('size of vocabulary: ' + str(sizeVocab) + '\\n\\n')\n",
    "\n",
    "#--(g)The number of word-tokens in each class--#\n",
    "performance_file.write('(g)\\n')\n",
    "#get word frequency per class\n",
    "wordFredPerClass = multinomialNaiveBayes.feature_count_\n",
    "performance_file.write('Number of words in business: ' + str(wordFredPerClass[0].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in entertainment: ' + str(wordFredPerClass[1].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in politics: ' + str(wordFredPerClass[2].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in sport: ' + str(wordFredPerClass[3].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in tech: ' + str(wordFredPerClass[4].sum(axis=0)) + '\\n\\n')\n",
    "\n",
    "#--(h)The number of word-tokens in the entire corpus--#\n",
    "performance_file.write('(h)\\n')\n",
    "wordFreq = cv_fit.toarray().sum(axis=0)\n",
    "performance_file.write('The number of word tokens for the entire corpus: ' + str(wordFreq.sum(axis=0)) + '\\n\\n')\n",
    "\n",
    "#--(i)The number and percentage of words with a frequency of zero in each class--#\n",
    "performance_file.write('(i)\\n')\n",
    "\n",
    "#count business\n",
    "\n",
    "#We know how many words are in business \n",
    "wordsInBusiness = wordFredPerClass[0].sum(axis=0)\n",
    "\n",
    "#And we can find the amount of zeros through subtraction\n",
    "#Take amount of articles x size of the vocabulary\n",
    "#Then subtract it by the amount of words that show up in business\n",
    "totalAmtWords = 510 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInBusiness\n",
    "performance_file.write('Number of words in business with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in business with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count entertainment\n",
    "wordsInEntertainment = wordFredPerClass[1].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 386 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInEntertainment\n",
    "performance_file.write('Number of words in entertainment with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in entertainment with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count politics\n",
    "wordsInPolitics = wordFredPerClass[2].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 417 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInPolitics\n",
    "performance_file.write('Number of words in politics with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in politics with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count sport\n",
    "wordsInSport = wordFredPerClass[3].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 511 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInSport\n",
    "performance_file.write('Number of words in sport with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in sport with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count tech\n",
    "wordsInTech = wordFredPerClass[4].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 401 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInTech\n",
    "performance_file.write('Number of words in tech with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in tech with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#--(j)The number and percentage of words with a frequency of one in the entire corpus--#\n",
    "performance_file.write('(j)\\n')\n",
    "performance_file.write('Number of words with frequency 1: ' + str(np.count_nonzero(cv_fit.toarray() == 1)) + \"\\n\")\n",
    "totalAmtWords = 2225 * sizeVocab\n",
    "performance_file.write('Percentage of words with frequency 1: ' + str(np.count_nonzero(cv_fit.toarray() == 1) / totalAmtWords) + \"\\n\\n\")\n",
    "\n",
    "#--(k)Your 2 favorite words (that are present in the vocabulary) and their log-prob--#\n",
    "performance_file.write('(k)\\n')\n",
    "#Sum up all words\n",
    "result = [sum(x) for x in zip(*cv_fit.toarray())]\n",
    "\n",
    "#Find the frequency of favorite words\n",
    "#'athletic' has index 3000\n",
    "athletic = result[3000]\n",
    "#'goalscorer' has index 12000\n",
    "goalscorer = result[12000]\n",
    "\n",
    "#Find probability of each word\n",
    "athleticProb = athletic/totalAmtWords\n",
    "goalscorerProb = goalscorer/totalAmtWords\n",
    "\n",
    "#Find log probability of each word\n",
    "performance_file.write('Log Probability of\\'athletic\\': ' + str(math.log(10,athleticProb)) + \"\\n\")\n",
    "performance_file.write('Log Probability of\\'athletic\\': ' + str(math.log(10,goalscorerProb)) + \"\\n\\n\")\n",
    "\n",
    "#-------------Step6: Train a multinomial Naive Bayes Classifier on the training set-------------#\n",
    "multinomialNaiveBayes = MultinomialNB(alpha = 0.0001)\n",
    "multinomialNaiveBayes.fit(X_train, y_train)\n",
    "y_predict = multinomialNaiveBayes.predict(X_test)\n",
    "\n",
    "#-------------Step7: BBC Performance-------------#\n",
    "\n",
    "#--(a)A clear seperator describing the model--#\n",
    "performance_file.write('-------------------------------------\\n')\n",
    "performance_file.write('Multi-nomialNB default values, try 3 smooting 0.0001\\n')\n",
    "performance_file.write('-------------------------------------\\n')\n",
    "\n",
    "#--(b)Confusion Matrix--#\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "performance_file.write('(b)\\n')\n",
    "for item in cm:\n",
    "        performance_file.write(\"%s\\n\" % item)\n",
    "performance_file.write('\\n')\n",
    "\n",
    "#--(c)Find Precision, Recall, F1-measure for each class--#\n",
    "target_names = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "performance_file.write('(c)\\n')\n",
    "performance_file.write(classification_report(y_test, y_predict, target_names=target_names))\n",
    "performance_file.write('\\n')\n",
    "\n",
    "#--(d)Find Accuracy, Macro-Average F1 and Weighted-Average F1 of the moodel--#\n",
    "accuracy = str(accuracy_score(y_test, y_predict))\n",
    "performance_file.write('(d)\\n')\n",
    "performance_file.write('accuracy: ' + accuracy + \"\\n\")\n",
    "\n",
    "macro = str(f1_score(y_test, y_predict, average='macro'))\n",
    "performance_file.write('macro-average F1: ' + macro + \"\\n\")\n",
    "\n",
    "weighted = str(f1_score(y_test, y_predict, average='weighted'))\n",
    "performance_file.write('weighted-average F1: ' + weighted + \"\\n\\n\")\n",
    "\n",
    "#--(e)Find prior probability of each class--#\n",
    "priors = multinomialNaiveBayes.class_log_prior_\n",
    "performance_file.write('(e)\\n')\n",
    "performance_file.write(\"\\nP(business)= \" + str(priors[0]))\n",
    "performance_file.write(\"\\nP(entertainment)= \" + str(priors[1]))\n",
    "performance_file.write(\"\\nP(politics)= \" + str(priors[2]))\n",
    "performance_file.write(\"\\nP(sport)= \" + str(priors[3]))\n",
    "performance_file.write(\"\\nP(tech)= \" + str(priors[4]))\n",
    "performance_file.write('\\n\\n')\n",
    "\n",
    "#--(f)The size of the vocabulary--#\n",
    "performance_file.write('(f)\\n')\n",
    "sizeVocab = multinomialNaiveBayes.n_features_in_\n",
    "performance_file.write('size of vocabulary: ' + str(sizeVocab) + '\\n\\n')\n",
    "\n",
    "#--(g)The number of word-tokens in each class--#\n",
    "performance_file.write('(g)\\n')\n",
    "#get word frequency per class\n",
    "wordFredPerClass = multinomialNaiveBayes.feature_count_\n",
    "performance_file.write('Number of words in business: ' + str(wordFredPerClass[0].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in entertainment: ' + str(wordFredPerClass[1].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in politics: ' + str(wordFredPerClass[2].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in sport: ' + str(wordFredPerClass[3].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in tech: ' + str(wordFredPerClass[4].sum(axis=0)) + '\\n\\n')\n",
    "\n",
    "#--(h)The number of word-tokens in the entire corpus--#\n",
    "performance_file.write('(h)\\n')\n",
    "wordFreq = cv_fit.toarray().sum(axis=0)\n",
    "performance_file.write('The number of word tokens for the entire corpus: ' + str(wordFreq.sum(axis=0)) + '\\n\\n')\n",
    "\n",
    "#--(i)The number and percentage of words with a frequency of zero in each class--#\n",
    "performance_file.write('(i)\\n')\n",
    "\n",
    "#count business\n",
    "\n",
    "#We know how many words are in business \n",
    "wordsInBusiness = wordFredPerClass[0].sum(axis=0)\n",
    "\n",
    "#And we can find the amount of zeros through subtraction\n",
    "#Take amount of articles x size of the vocabulary\n",
    "#Then subtract it by the amount of words that show up in business\n",
    "totalAmtWords = 510 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInBusiness\n",
    "performance_file.write('Number of words in business with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in business with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count entertainment\n",
    "wordsInEntertainment = wordFredPerClass[1].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 386 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInEntertainment\n",
    "performance_file.write('Number of words in entertainment with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in entertainment with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count politics\n",
    "wordsInPolitics = wordFredPerClass[2].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 417 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInPolitics\n",
    "performance_file.write('Number of words in politics with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in politics with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count sport\n",
    "wordsInSport = wordFredPerClass[3].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 511 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInSport\n",
    "performance_file.write('Number of words in sport with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in sport with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count tech\n",
    "wordsInTech = wordFredPerClass[4].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 401 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInTech\n",
    "performance_file.write('Number of words in tech with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in tech with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#--(j)The number and percentage of words with a frequency of one in the entire corpus--#\n",
    "performance_file.write('(j)\\n')\n",
    "performance_file.write('Number of words with frequency 1: ' + str(np.count_nonzero(cv_fit.toarray() == 1)) + \"\\n\")\n",
    "totalAmtWords = 2225 * sizeVocab\n",
    "performance_file.write('Percentage of words with frequency 1: ' + str(np.count_nonzero(cv_fit.toarray() == 1) / totalAmtWords) + \"\\n\\n\")\n",
    "\n",
    "#--(k)Your 2 favorite words (that are present in the vocabulary) and their log-prob--#\n",
    "performance_file.write('(k)\\n')\n",
    "#Sum up all words\n",
    "result = [sum(x) for x in zip(*cv_fit.toarray())]\n",
    "\n",
    "#Find the frequency of favorite words\n",
    "#'athletic' has index 3000\n",
    "athletic = result[3000]\n",
    "#'goalscorer' has index 12000\n",
    "goalscorer = result[12000]\n",
    "\n",
    "#Find probability of each word\n",
    "athleticProb = athletic/totalAmtWords\n",
    "goalscorerProb = goalscorer/totalAmtWords\n",
    "\n",
    "#Find log probability of each word\n",
    "performance_file.write('Log Probability of\\'athletic\\': ' + str(math.log(10,athleticProb)) + \"\\n\")\n",
    "performance_file.write('Log Probability of\\'athletic\\': ' + str(math.log(10,goalscorerProb)) + \"\\n\\n\")\n",
    "\n",
    "#-------------Step6: Train a multinomial Naive Bayes Classifier on the training set-------------#\n",
    "multinomialNaiveBayes = MultinomialNB(alpha = 0.9)\n",
    "multinomialNaiveBayes.fit(X_train, y_train)\n",
    "y_predict = multinomialNaiveBayes.predict(X_test)\n",
    "\n",
    "#-------------Step7: BBC Performance-------------#\n",
    "\n",
    "#--(a)A clear seperator describing the model--#\n",
    "performance_file.write('-------------------------------------\\n')\n",
    "performance_file.write('Multi-nomialNB default values, try 4 (smoothing 0.9)\\n')\n",
    "performance_file.write('-------------------------------------\\n')\n",
    "\n",
    "#--(b)Confusion Matrix--#\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "performance_file.write('(b)\\n')\n",
    "for item in cm:\n",
    "        performance_file.write(\"%s\\n\" % item)\n",
    "performance_file.write('\\n')\n",
    "\n",
    "#--(c)Find Precision, Recall, F1-measure for each class--#\n",
    "target_names = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "performance_file.write('(c)\\n')\n",
    "performance_file.write(classification_report(y_test, y_predict, target_names=target_names))\n",
    "performance_file.write('\\n')\n",
    "\n",
    "#--(d)Find Accuracy, Macro-Average F1 and Weighted-Average F1 of the moodel--#\n",
    "accuracy = str(accuracy_score(y_test, y_predict))\n",
    "performance_file.write('(d)\\n')\n",
    "performance_file.write('accuracy: ' + accuracy + \"\\n\")\n",
    "\n",
    "macro = str(f1_score(y_test, y_predict, average='macro'))\n",
    "performance_file.write('macro-average F1: ' + macro + \"\\n\")\n",
    "\n",
    "weighted = str(f1_score(y_test, y_predict, average='weighted'))\n",
    "performance_file.write('weighted-average F1: ' + weighted + \"\\n\\n\")\n",
    "\n",
    "#--(e)Find prior probability of each class--#\n",
    "priors = multinomialNaiveBayes.class_log_prior_\n",
    "performance_file.write('(e)\\n')\n",
    "performance_file.write(\"\\nP(business)= \" + str(priors[0]))\n",
    "performance_file.write(\"\\nP(entertainment)= \" + str(priors[1]))\n",
    "performance_file.write(\"\\nP(politics)= \" + str(priors[2]))\n",
    "performance_file.write(\"\\nP(sport)= \" + str(priors[3]))\n",
    "performance_file.write(\"\\nP(tech)= \" + str(priors[4]))\n",
    "performance_file.write('\\n\\n')\n",
    "\n",
    "#--(f)The size of the vocabulary--#\n",
    "performance_file.write('(f)\\n')\n",
    "sizeVocab = multinomialNaiveBayes.n_features_in_\n",
    "performance_file.write('size of vocabulary: ' + str(sizeVocab) + '\\n\\n')\n",
    "\n",
    "#--(g)The number of word-tokens in each class--#\n",
    "performance_file.write('(g)\\n')\n",
    "#get word frequency per class\n",
    "wordFredPerClass = multinomialNaiveBayes.feature_count_\n",
    "performance_file.write('Number of words in business: ' + str(wordFredPerClass[0].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in entertainment: ' + str(wordFredPerClass[1].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in politics: ' + str(wordFredPerClass[2].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in sport: ' + str(wordFredPerClass[3].sum(axis=0)) + '\\n')\n",
    "performance_file.write('Number of words in tech: ' + str(wordFredPerClass[4].sum(axis=0)) + '\\n\\n')\n",
    "\n",
    "#--(h)The number of word-tokens in the entire corpus--#\n",
    "performance_file.write('(h)\\n')\n",
    "wordFreq = cv_fit.toarray().sum(axis=0)\n",
    "performance_file.write('The number of word tokens for the entire corpus: ' + str(wordFreq.sum(axis=0)) + '\\n\\n')\n",
    "\n",
    "#--(i)The number and percentage of words with a frequency of zero in each class--#\n",
    "performance_file.write('(i)\\n')\n",
    "\n",
    "#count business\n",
    "\n",
    "#We know how many words are in business \n",
    "wordsInBusiness = wordFredPerClass[0].sum(axis=0)\n",
    "\n",
    "#And we can find the amount of zeros through subtraction\n",
    "#Take amount of articles x size of the vocabulary\n",
    "#Then subtract it by the amount of words that show up in business\n",
    "totalAmtWords = 510 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInBusiness\n",
    "performance_file.write('Number of words in business with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in business with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count entertainment\n",
    "wordsInEntertainment = wordFredPerClass[1].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 386 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInEntertainment\n",
    "performance_file.write('Number of words in entertainment with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in entertainment with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count politics\n",
    "wordsInPolitics = wordFredPerClass[2].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 417 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInPolitics\n",
    "performance_file.write('Number of words in politics with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in politics with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count sport\n",
    "wordsInSport = wordFredPerClass[3].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 511 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInSport\n",
    "performance_file.write('Number of words in sport with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in sport with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#count tech\n",
    "wordsInTech = wordFredPerClass[4].sum(axis=0)\n",
    "\n",
    "totalAmtWords = 401 * sizeVocab\n",
    "frqZero = totalAmtWords - wordsInTech\n",
    "performance_file.write('Number of words in tech with frequency zero: ' + str(frqZero) + \"\\n\")\n",
    "performance_file.write('Percentage of words in tech with frequency zero: ' + str(frqZero / totalAmtWords) + \"%\\n\\n\")\n",
    "\n",
    "#--(j)The number and percentage of words with a frequency of one in the entire corpus--#\n",
    "performance_file.write('(j)\\n')\n",
    "performance_file.write('Number of words with frequency 1: ' + str(np.count_nonzero(cv_fit.toarray() == 1)) + \"\\n\")\n",
    "totalAmtWords = 2225 * sizeVocab\n",
    "performance_file.write('Percentage of words with frequency 1: ' + str(np.count_nonzero(cv_fit.toarray() == 1) / totalAmtWords) + \"\\n\\n\")\n",
    "\n",
    "#--(k)Your 2 favorite words (that are present in the vocabulary) and their log-prob--#\n",
    "performance_file.write('(k)\\n')\n",
    "#Sum up all words\n",
    "result = [sum(x) for x in zip(*cv_fit.toarray())]\n",
    "\n",
    "#Find the frequency of favorite words\n",
    "#'athletic' has index 3000\n",
    "athletic = result[3000]\n",
    "#'goalscorer' has index 12000\n",
    "goalscorer = result[12000]\n",
    "\n",
    "#Find probability of each word\n",
    "athleticProb = athletic/totalAmtWords\n",
    "goalscorerProb = goalscorer/totalAmtWords\n",
    "\n",
    "#Find log probability of each word\n",
    "performance_file.write('Log Probability of\\'athletic\\': ' + str(math.log(10,athleticProb)) + \"\\n\")\n",
    "performance_file.write('Log Probability of\\'athletic\\': ' + str(math.log(10,goalscorerProb)) + \"\\n\\n\")\n",
    "\n",
    "#close file\n",
    "performance_file.close()\n",
    "\n",
    "#--Step 8\n",
    "open(\"bbc-discussion.txt\", \"w\").close()\n",
    "discussion_file = open('bbc-discussion.txt', 'a')\n",
    "\n",
    "discussion_file.write('The distribution of articles amongst the classes is relatively balanced. This means that each class is well represented in this dataset\\n' + \n",
    "                     ('and carries equal importance. As such, accuracy is more well-suited for this dataset (over Recall, Precision & F-measure).\\n\\n'))\n",
    "\n",
    "discussion_file.write('The performance of steps 8-10 differed from one another when we involved data that was predicted by the classifier. This means that\\n' +\n",
    "                     ('metrics like Recall, Precision, F1-measure, etc... were different because the results of the prediction changed after each trial.\\n') +\n",
    "                     ('On the other hand, steps that involve the entire corpus like vocabulary size (f) or number of words with frequency 1 over the entire\\n') +\n",
    "                     ('corpus (j) would remain the same from one trial to another.'))\n",
    "\n",
    "discussion_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0f8e7-07c5-4827-876e-b73fcd11f404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
