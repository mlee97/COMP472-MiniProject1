For the NB, Base-DT, Top-DT and PER classifiers, the same model gives the same performance every time. For the NB classifier, we are reusing the same splitted data (ie. the training data is always the same and the testing data as well). Since we are using the same training data for the 10 iterations to perform training, we will always get the same results when computing the prior and conditional probabilities. Thus, we will always get the same results during testing. This is the reason the performance stays the same throughout the 10 iterations. For the DT and Top-DT classifier, the most driscriminating feature and the information gains will always be the same throughout the 10 iterations since we are using the same training data. Thus, even if the tree structure is different, the output will remain the same. For the perceptron, because we are always using the same learning rate, the algorithm to learn the weights of each input would result in the same weights each time. Which is the reason why the classifier has the same performance each time.

For the Base-MLP and Top-MLP, the performance is slightly different every time. This is because the method used (MLPClassifier()) has a 'random_state = none' parameter, which generates random weights and bias for each iteration. Since the weights and bias is different for each iteration, the output will be different. Thus, it results in different performances.